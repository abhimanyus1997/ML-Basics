{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of NLP (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tasks in NLP:\n",
    "    - translation, \n",
    "    - automatic summarization, \n",
    "    - Named Entity Recognition (NER), \n",
    "    - speech recognition, \n",
    "    - relationship extraction, and \n",
    "    - topic segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps in NLP Pipeline:**   \n",
    "1. Sentence Segementation  \n",
    "2. Word Tokenization  \n",
    "3. Stemming  \n",
    "4. Lemmatization  \n",
    "5. Identify Stop Words  \n",
    "6. Dependency Parsing  \n",
    "7. POS (Part of Speech) Tagging  \n",
    "8. Named Entity Recognition (NER)  \n",
    "9. Chunking  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difficulties in NLP:\n",
    " * Ambiguity\n",
    "    * Lexical Ambiguity : noun adj or verb\n",
    "    * Syntactic Ambiguity\n",
    "    * Referential Ambiguity\n",
    " * Lack of Context\n",
    " * Named Entity Recognition (NER):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splits text by word/sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    " from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "example_string = \"\"\"\n",
    "Natural language processing (NLP) is a field of computer science that deals with the interaction between computers and human (natural) languages. It's a subfield of artificial intelligence that deals with the ability of computers to understand and process human language, including speech and text.\n",
    "NLP has many applications, including machine translation, speech recognition, text analysis, and question answering. It's used in a variety of industries, including healthcare, finance, and customer service.\n",
    "One of the most important tasks in NLP is to understand the meaning of text. This can be challenging because words can have multiple meanings, and the meaning of a sentence can depend on the context in which it's used. NLP systems use a variety of techniques to understand meaning, including詞法分析, 語法分析, and 語義分析.\n",
    "Another important task in NLP is to generate text. This can be challenging because it requires the system to understand the meaning of the text it's generating and to be able to express that meaning in a way that is both grammatically correct and fluent. NLP systems use a variety of techniques to generate text, including machine translation, text summarization, and question answering.\n",
    "NLP is a rapidly growing field, and it's having a major impact on the way we interact with computers. As NLP systems become more sophisticated, they'll be able to understand and process human language in ways that are currently unimaginable. This will lead to new and innovative applications in a variety of industries.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nNatural language processing (NLP) is a field of computer science that deals with the interaction between computers and human (natural) languages.',\n",
       " \"It's a subfield of artificial intelligence that deals with the ability of computers to understand and process human language, including speech and text.\",\n",
       " 'NLP has many applications, including machine translation, speech recognition, text analysis, and question answering.',\n",
       " \"It's used in a variety of industries, including healthcare, finance, and customer service.\",\n",
       " 'One of the most important tasks in NLP is to understand the meaning of text.',\n",
       " \"This can be challenging because words can have multiple meanings, and the meaning of a sentence can depend on the context in which it's used.\",\n",
       " 'NLP systems use a variety of techniques to understand meaning, including詞法分析, 語法分析, and 語義分析.',\n",
       " 'Another important task in NLP is to generate text.',\n",
       " \"This can be challenging because it requires the system to understand the meaning of the text it's generating and to be able to express that meaning in a way that is both grammatically correct and fluent.\",\n",
       " 'NLP systems use a variety of techniques to generate text, including machine translation, text summarization, and question answering.',\n",
       " \"NLP is a rapidly growing field, and it's having a major impact on the way we interact with computers.\",\n",
       " \"As NLP systems become more sophisticated, they'll be able to understand and process human language in ways that are currently unimaginable.\",\n",
       " 'This will lead to new and innovative applications in a variety of industries.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence tokenizer\n",
    "sent_tokenize(example_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'field',\n",
       " 'of',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'that',\n",
       " 'deals',\n",
       " 'with',\n",
       " 'the',\n",
       " 'interaction',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'human',\n",
       " '(',\n",
       " 'natural',\n",
       " ')',\n",
       " 'languages',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'subfield',\n",
       " 'of',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'that',\n",
       " 'deals',\n",
       " 'with',\n",
       " 'the',\n",
       " 'ability',\n",
       " 'of',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'and',\n",
       " 'process',\n",
       " 'human',\n",
       " 'language',\n",
       " ',',\n",
       " 'including',\n",
       " 'speech',\n",
       " 'and']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Tokenizer\n",
    "word_tokenize(example_string)[:50] # Prinying first 50 only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that you want to ignore ex comman words like 'is', 'an', 'the' etc. as they are not that meaningful in some tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'better', 'to', 'die', 'than', 'to', 'preserve', 'this', 'life', 'by', 'incurring', 'disgrace', '.', 'The', 'loss', 'of', 'life', 'causes', 'but', 'a', 'moment', \"'s\", 'grief', ',', 'but', 'disgrace', 'brings', 'grief', 'every', 'day', 'of', 'one', \"'s\", 'life', '.']\n"
     ]
    }
   ],
   "source": [
    "chankya_quote = \"It is better to die than to preserve this life by incurring disgrace. The loss of life causes but a moment's grief, but disgrace brings grief every day of one's life.\"\n",
    "words_in_quote = word_tokenize(chankya_quote)\n",
    "print(words_in_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a set of stopwords in english\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['better', 'die', 'preserve', 'life', 'incurring', 'disgrace', '.', 'loss', 'life', 'causes', 'moment', \"'s\", 'grief', ',', 'disgrace', 'brings', 'grief', 'every', 'day', 'one', \"'s\", 'life', '.']\n"
     ]
    }
   ],
   "source": [
    "# METHOD 1 To FILTER\n",
    "filtered_list = [] # Holds non-stopwords\n",
    "for word in words_in_quote:\n",
    "   if word.casefold() not in stop_words: #casefold() ignores the case\n",
    "       filtered_list.append(word)\n",
    "\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['better', 'die', 'preserve', 'life', 'incurring', 'disgrace', '.', 'loss', 'life', 'causes', 'moment', \"'s\", 'grief', ',', 'disgrace', 'brings', 'grief', 'every', 'day', 'one', \"'s\", 'life', '.']\n"
     ]
    }
   ],
   "source": [
    "# METHOD 2 : List Comprehesion filter out stopword\n",
    "filtered_list = [\n",
    "    word for word in words_in_quote if word.casefold() not in stop_words\n",
    "]\n",
    "print(filtered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is a task to reduce the word to root word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stemmer available in nltk are:\n",
    "* Porter stemmer\n",
    "* Snowball stemmer\n",
    "* ARLSTem Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter Stemmer in NLTK\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'better', 'to', 'die', 'than', 'to', 'preserv', 'thi', 'life', 'by', 'incur', 'disgrac', '.', 'the', 'loss', 'of', 'life', 'caus', 'but', 'a', 'moment', \"'s\", 'grief', ',', 'but', 'disgrac', 'bring', 'grief', 'everi', 'day', 'of', 'one', \"'s\", 'life', '.']\n"
     ]
    }
   ],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in words_in_quote]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem with stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understemming and overstemming are two ways stemming can go wrong:\n",
    "\n",
    "1. **Understemming** happens when two related words should be reduced to the same stem but aren’t. This is a false negative.\n",
    "\n",
    "1. **Overstemming** happens when two unrelated words are reduced to the same stem even though they shouldn’t be. This is a false positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snowball Stemmer (Porter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few Rules:  \n",
    "* ILY  -----> ILI  \n",
    "* LY   ----->   \n",
    "* SS   -----> SS  \n",
    "* S    ----->   \n",
    "* ED   -----> E,Nil  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'better', 'to', 'die', 'than', 'to', 'preserv', 'this', 'life', 'by', 'incur', 'disgrac', '.', 'the', 'loss', 'of', 'life', 'caus', 'but', 'a', 'moment', \"'s\", 'grief', ',', 'but', 'disgrac', 'bring', 'grief', 'everi', 'day', 'of', 'one', \"'s\", 'life', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "stemmed_words = [snow_stemmer.stem(word) for word in words_in_quote]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **lemma** is a word that represents a whole group of words, and that group of words is called a **lexeme**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing reduces words to their core meaning, but it will give you a complete English word that makes sense on its own instead of just a fragment of a word like 'discoveri'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scarf'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"scarves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'friend', 'of', 'DeSoto', 'love', 'scarf', '.']\n"
     ]
    }
   ],
   "source": [
    "string_for_lemmatizing = \"The friends of DeSoto love scarves.\"\n",
    "words = word_tokenize(string_for_lemmatizing)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
